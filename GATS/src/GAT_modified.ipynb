{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora_content:\n",
      "       0     1     2     3     4     5     6     7     8     9     ...  1425  \\\n",
      "0    31336     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1  1061127     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2  1106406     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3    13195     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4    37879     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1426  1427  1428  1429  1430  1431  1432  1433                    1434  \n",
      "0     0     1     0     0     0     0     0     0         Neural_Networks  \n",
      "1     1     0     0     0     0     0     0     0           Rule_Learning  \n",
      "2     0     0     0     0     0     0     0     0  Reinforcement_Learning  \n",
      "3     0     0     0     0     0     0     0     0  Reinforcement_Learning  \n",
      "4     0     0     0     0     0     0     0     0   Probabilistic_Methods  \n",
      "\n",
      "[5 rows x 1435 columns]\n",
      "cora_cites:\n",
      "     0        1\n",
      "0  35     1033\n",
      "1  35   103482\n",
      "2  35   103515\n",
      "3  35  1050679\n",
      "4  35  1103960\n"
     ]
    }
   ],
   "source": [
    "cora_content_path = '../dataset/cora/cora.content'\n",
    "cora_content = pd.read_csv(cora_content_path, sep='\\t', header=None)\n",
    "cora_cites_path = '../dataset/cora/cora.cites'\n",
    "cora_cites = pd.read_csv(cora_cites_path, sep='\\t', header=None)\n",
    "\n",
    "# 打印数据的前5行\n",
    "print(\"cora_content:\\n\",cora_content.head())\n",
    "print(\"cora_cites:\\n\",cora_cites.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义辅助函数热独编码,这里的labels是一个列表\n",
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels))) # set去重，sorted按照从小到大排序\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)} # 生成热独编码字典 {类别：热独编码}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32) # map函数，对labels中的每个元素进行热独编码,get函数单独使用，返回字典中key对应的value\n",
    "    return labels_onehot # 返回热独编码后的标签,shape为(N,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数据归一化,输入维度为(N,1433)\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)) # 按行求和，shape为(N,1)\n",
    "    r_inv = np.power(rowsum, -1).flatten() # 求倒数,flatten()将多维数组降为一维\n",
    "    r_inv[np.isinf(r_inv)] = 0. # 将无穷大的值设置为0\n",
    "    r_mat_inv = sp.diags(r_inv) # 对角矩阵\n",
    "    mx = r_mat_inv.dot(mx) # 矩阵相乘，实现归一化，shape为(N,F)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邻接矩阵数据归一化\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)) # 按行求和，shape为(N,1)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten() # 求倒数的平方根\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0. # 将无穷大的值设置为0\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt) # 对角矩阵\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt) # 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确率，输出的output的形状是(N,C),labels的形状是(N,)，其中N是样本数，C是类别数\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels) # 返回每行最大值的索引，即预测的类别\n",
    "    correct = preds.eq(labels).double() # 判断是否相等\n",
    "    correct = correct.sum() # 统计相等的个数\n",
    "    return correct / len(labels) # 返回准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征以及标签读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "idx_features_labels = np.genfromtxt(cora_content_path, dtype=np.dtype(str)) # 读取数据\n",
    "# 从数据中提取特征\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # 提取特征\n",
    "# 提取标签\n",
    "labels = encode_onehot(idx_features_labels[:, -1]) # 提取标签,-1表示最后一列，同时进行热独编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成图状结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成图结构\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # 提取索引\n",
    "idx_map = {j: i for i, j in enumerate(idx)} # 生成索引字典,{原索引：新索引},enumerate将列表转换为索引-值对\n",
    "# 生成边信息\n",
    "edges_unordered = np.genfromtxt(cora_cites_path, dtype=np.int32) # 读取边信息，shape=(5429,2)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape) # 生成重新编号的边信息，shape=(5429,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32) # 生成邻接矩阵的稀疏矩阵，np.ones(edges.shape[0])表示矩阵中所有非零元素的值，edges[:, 0]和edges[:, 1]分别表示这些非零元素的行索引和列索引。\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) # 对称归一化邻接矩阵,因为无向图，所以邻接矩阵应该是对称的，这段代码将adj更新为adj和adj.T中的较大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = normalize_features(features) # 特征归一化\n",
    "adj = normalize_adj(adj + sp.eye(adj.shape[0])) # 邻接矩阵归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分成训练集、验证集和测试集\n",
    "idx_train = range(140) # 训练集\n",
    "idx_val = range(200, 500) # 验证集\n",
    "idx_test = range(500, 1500) # 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据转换成能导入模型的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.FloatTensor(np.array(adj.todense())) # 将邻接矩阵转换为tensor\n",
    "features = torch.FloatTensor(np.array(features.todense())) # 将特征转换为tensor\n",
    "labels = torch.LongTensor(np.where(labels)[1]) # 将标签转换为tensor\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train) # 将训练集索引转换为tensor\n",
    "idx_val = torch.LongTensor(idx_val) # 将验证集索引转换为tensor\n",
    "idx_test = torch.LongTensor(idx_test) # 将测试集索引转换为tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步计算：\n",
    "$e_{ij}=a(\\mathbf{W}\\vec{h}_i,\\mathbf{W}\\vec{h}_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步计算：\n",
    "$\\alpha_{ij}=\\mathrm{softmax}_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\exp(e_{ik})}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合计算公式：\n",
    "$\\alpha_{ij}=\\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\mathbf{a}}^T[\\mathbf{W}\\vec{h}_i\\|\\mathbf{W}\\vec{h}_j]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i}\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\mathbf{a}}^T[\\mathbf{W}\\vec{h}_i\\|\\mathbf{W}\\vec{h}_k]\\right)\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h'计算公式：\n",
    "$\\vec{h}_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}\\vec{h}_j\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSingleLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat = True): # in_features是输入特征维度，out_features是输出特征维度，dropout是dropout的概率，alpha是leakyrelu的参数，concat是否要拼接\n",
    "        super(GSingleLayer, self).__init__()\n",
    "        self.dropout = dropout # 在数据量较少的时候，可以使用dropout防止过拟合\n",
    "        self.in_features = in_features # 输入特征维度F\n",
    "        self.out_features = out_features # 输出特征维度F'\n",
    "        self.concat = concat # 是否要拼接，对于中间层，需要拼接因为这里使用的是多头注意力，但是对于最后一层就不需要拼接直接计算平均值\n",
    "        self.alpha = alpha # leakyrelu的参数\n",
    "\n",
    "        # 定义权重矩阵\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))) # 定义权重矩阵,shape=[F,F'],这里跟论文中的不一样是后面在将数据导入模型的时候纬度是[N,F]。这里要相应进行调整\n",
    "        # 用Xavier方法初始化权重矩阵\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414) # 使用Xavier方法初始化权重矩阵\n",
    "\n",
    "        # 定义权重向量a\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1))) # 定义权重向量a,shape=[2F',1]\n",
    "        # 用Xavier方法初始化权重向量a\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    # 定义在这一层节点会进行什么操作\n",
    "    # h就是论文中的输入特征集合，adj就是邻接矩阵\n",
    "    def forward(self, h, adj):\n",
    "        # 首先是e的计算，矩阵的使用用高维的避免了循环遍历\n",
    "        Wh = torch.mm(h, self.W) # shape [N, F']\n",
    "        Wh1 = torch.matmul(Wh,self.a[:self.out_features, :]) # shape [N, 1], 这里将a拆开相当于后面将两个拼接起来\n",
    "        Wh2 = torch.matmul(Wh,self.a[self.out_features:, :]) # shape [N, 1]，这里一定要记住不能少了后面这个冒号！！！！！\n",
    "        e = self.leakyrelu(Wh1 + Wh2.T) # shape [N, N]，同时这里使用了广播相加\n",
    "\n",
    "        # 计算注意力系数a\n",
    "        zero_vec = -9e15*torch.ones_like(e) # 生成负无穷矩阵，方便后续经过softmax函数直接变成0\n",
    "        attention = torch.where(adj > 0, e, zero_vec) # 前面计算了所有节点之间的相关性系数，这里筛选出有边关系的保留attention,剩下的不保留\n",
    "        attention = F.softmax(attention, dim = 1)\n",
    "        attention = F.dropout(attention, self.dropout, training = self.training)\n",
    "        h_prime = torch.matmul(attention, Wh) # 将所有的attention和Wh相乘得到当前层的输出\n",
    "\n",
    "        if self.concat: # 如果是拼接的话\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Network(nn.Module):\n",
    "    def __init__(self, nfeature, nhidden, nclass, dropout, alpha, nheads): # nfeature是输入特征的维度，nhidden是隐藏层的维度，nclass是输出的类别数，dropout是dropout的概率，alpha是leakyrelu的参数，nheads是多头注意力的头数\n",
    "        super(GAT_Network, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads \n",
    "        # 定义多个注意力层\n",
    "        self.attentions = [GSingleLayer(nfeature, nhidden, dropout, alpha) for _ in range(nheads)] # 定义多个注意力层，输出是[nhidden, nheads]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            # 将多个注意力层加入到模型中，并行计算\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        # 定义输出层，输出层的输入维度就是上一层[nhidden, nheads]，由于这里是最后一层，根据论文中说的就不进行拼接了\n",
    "        self.out_att = GSingleLayer(nhidden * nheads, nclass, dropout, alpha, concat = False)\n",
    "    \n",
    "    # 定义在网络架构中信息的传递规则\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training = self.training) # dropout\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim = 1) # 将多个注意力层的输出拼接起来\n",
    "        x = F.dropout(x, self.dropout, training = self.training)\n",
    "        x = F.elu(self.out_att(x, adj)) # 用elu激活函数\n",
    "        return F.log_softmax(x, dim = 1) # softmax将输出转换为概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练以及评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子,保证实验的可重复性\n",
    "seed = 72\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# 定义训练参数序列\n",
    "param = {\n",
    "\n",
    "    \"epochs\": 200, # \"Number of epochs to train.\"\n",
    "    'lr': 0.005,  # \"Initial learning rate.\"\n",
    "    'weight_decay': 5e-4,  # \"Weight decay (L2 loss on parameters).\"\n",
    "    'hidden': 8,  # \"Number of hidden units.\"\n",
    "    'nb_heads': 8,  # \"Number of head attentions.\"\n",
    "    'dropout': 0.6,  # \"Dropout rate (1 - keep probability).\"\n",
    "    'alpha': 0.2,  # \"Alpha for the leaky_relu.\"\n",
    "    'patience': 100,    # \"Patience\"\n",
    "    'fastmode': False  # \"Activate fast mode (i.e. validate during training).\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT_Network(\n",
    "    nfeature = features.shape[1],\n",
    "    nhidden = param[\"hidden\"],\n",
    "    nclass = labels.max().item() + 1,\n",
    "    dropout = param[\"dropout\"],\n",
    "    alpha = param[\"alpha\"],\n",
    "    nheads = param[\"nb_heads\"]\n",
    ")\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), # 模型参数\n",
    "    lr = param[\"lr\"],\n",
    "    weight_decay = param[\"weight_decay\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将所有信息加载到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433]) torch.Size([2708, 2708]) torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "# 将数据转换为Variable,目的是为了计算梯度\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels) \n",
    "# 打印数据形状\n",
    "print(features.shape,adj.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # 训练模型\n",
    "    model.train() # 训练模式\n",
    "    optimizer.zero_grad() # 梯度清零\n",
    "    output = model(features, adj) # 前向传播\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) # 计算损失\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train]) # 计算准确率\n",
    "    loss_train.backward() # 反向传播\n",
    "    optimizer.step() # 更新参数\n",
    "\n",
    "    if not param[\"fastmode\"]: # 如果不是快速模式,验证集和测试集也要进行训练\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "        'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() # 测试模式\n",
    "    output = model(features, adj) # 前向传播\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) # 计算损失\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test]) # 计算准确率\n",
    "    print(\"Test set results:\",\n",
    "        \"loss= {:.4f}\".format(loss_test.item()),\n",
    "        \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9484 acc_train: 0.0929 loss_val: 1.9408 acc_val: 0.2833 time: 0.1246s\n",
      "Epoch: 0002 loss_train: 1.9371 acc_train: 0.2071 loss_val: 1.9315 acc_val: 0.5367 time: 0.0930s\n",
      "Epoch: 0003 loss_train: 1.9255 acc_train: 0.2786 loss_val: 1.9219 acc_val: 0.6467 time: 0.0941s\n",
      "Epoch: 0004 loss_train: 1.9129 acc_train: 0.4571 loss_val: 1.9122 acc_val: 0.6433 time: 0.0911s\n",
      "Epoch: 0005 loss_train: 1.9053 acc_train: 0.4429 loss_val: 1.9027 acc_val: 0.6400 time: 0.0941s\n",
      "Epoch: 0006 loss_train: 1.8876 acc_train: 0.5429 loss_val: 1.8933 acc_val: 0.6500 time: 0.0915s\n",
      "Epoch: 0007 loss_train: 1.8828 acc_train: 0.5500 loss_val: 1.8838 acc_val: 0.6500 time: 0.0915s\n",
      "Epoch: 0008 loss_train: 1.8656 acc_train: 0.5714 loss_val: 1.8740 acc_val: 0.6333 time: 0.0913s\n",
      "Epoch: 0009 loss_train: 1.8482 acc_train: 0.5500 loss_val: 1.8640 acc_val: 0.6267 time: 0.0910s\n",
      "Epoch: 0010 loss_train: 1.8381 acc_train: 0.5714 loss_val: 1.8539 acc_val: 0.6267 time: 0.0952s\n",
      "Epoch: 0011 loss_train: 1.8292 acc_train: 0.5429 loss_val: 1.8436 acc_val: 0.6200 time: 0.0913s\n",
      "Epoch: 0012 loss_train: 1.8222 acc_train: 0.5429 loss_val: 1.8331 acc_val: 0.6200 time: 0.0941s\n",
      "Epoch: 0013 loss_train: 1.8241 acc_train: 0.5500 loss_val: 1.8224 acc_val: 0.6133 time: 0.0911s\n",
      "Epoch: 0014 loss_train: 1.8051 acc_train: 0.5714 loss_val: 1.8116 acc_val: 0.6100 time: 0.0924s\n",
      "Epoch: 0015 loss_train: 1.8065 acc_train: 0.5357 loss_val: 1.8007 acc_val: 0.6033 time: 0.0923s\n",
      "Epoch: 0016 loss_train: 1.7682 acc_train: 0.5786 loss_val: 1.7895 acc_val: 0.6033 time: 0.0914s\n",
      "Epoch: 0017 loss_train: 1.7729 acc_train: 0.6071 loss_val: 1.7781 acc_val: 0.6100 time: 0.0940s\n",
      "Epoch: 0018 loss_train: 1.7533 acc_train: 0.5357 loss_val: 1.7666 acc_val: 0.6067 time: 0.0915s\n",
      "Epoch: 0019 loss_train: 1.7137 acc_train: 0.5786 loss_val: 1.7548 acc_val: 0.6067 time: 0.0940s\n",
      "Epoch: 0020 loss_train: 1.7092 acc_train: 0.5571 loss_val: 1.7428 acc_val: 0.6100 time: 0.0915s\n",
      "Epoch: 0021 loss_train: 1.6915 acc_train: 0.5857 loss_val: 1.7306 acc_val: 0.6100 time: 0.0942s\n",
      "Epoch: 0022 loss_train: 1.6650 acc_train: 0.6071 loss_val: 1.7182 acc_val: 0.6100 time: 0.0911s\n",
      "Epoch: 0023 loss_train: 1.6619 acc_train: 0.5786 loss_val: 1.7056 acc_val: 0.6200 time: 0.0924s\n",
      "Epoch: 0024 loss_train: 1.6518 acc_train: 0.5786 loss_val: 1.6930 acc_val: 0.6200 time: 0.0940s\n",
      "Epoch: 0025 loss_train: 1.6718 acc_train: 0.5786 loss_val: 1.6804 acc_val: 0.6300 time: 0.0914s\n",
      "Epoch: 0026 loss_train: 1.6161 acc_train: 0.6500 loss_val: 1.6676 acc_val: 0.6367 time: 0.0941s\n",
      "Epoch: 0027 loss_train: 1.6277 acc_train: 0.5500 loss_val: 1.6549 acc_val: 0.6367 time: 0.0911s\n",
      "Epoch: 0028 loss_train: 1.5759 acc_train: 0.5786 loss_val: 1.6420 acc_val: 0.6400 time: 0.0942s\n",
      "Epoch: 0029 loss_train: 1.5535 acc_train: 0.6000 loss_val: 1.6291 acc_val: 0.6400 time: 0.0924s\n",
      "Epoch: 0030 loss_train: 1.5886 acc_train: 0.5643 loss_val: 1.6162 acc_val: 0.6500 time: 0.0910s\n",
      "Epoch: 0031 loss_train: 1.5280 acc_train: 0.6071 loss_val: 1.6032 acc_val: 0.6500 time: 0.0915s\n",
      "Epoch: 0032 loss_train: 1.5334 acc_train: 0.6286 loss_val: 1.5902 acc_val: 0.6500 time: 0.0923s\n",
      "Epoch: 0033 loss_train: 1.5328 acc_train: 0.5643 loss_val: 1.5773 acc_val: 0.6500 time: 0.0951s\n",
      "Epoch: 0034 loss_train: 1.4740 acc_train: 0.7000 loss_val: 1.5643 acc_val: 0.6533 time: 0.0915s\n",
      "Epoch: 0035 loss_train: 1.5168 acc_train: 0.6357 loss_val: 1.5515 acc_val: 0.6567 time: 0.0945s\n",
      "Epoch: 0036 loss_train: 1.4830 acc_train: 0.6000 loss_val: 1.5387 acc_val: 0.6567 time: 0.0921s\n",
      "Epoch: 0037 loss_train: 1.4562 acc_train: 0.6357 loss_val: 1.5261 acc_val: 0.6600 time: 0.0922s\n",
      "Epoch: 0038 loss_train: 1.4578 acc_train: 0.6071 loss_val: 1.5135 acc_val: 0.6600 time: 0.0917s\n",
      "Epoch: 0039 loss_train: 1.4937 acc_train: 0.6000 loss_val: 1.5010 acc_val: 0.6633 time: 0.0931s\n",
      "Epoch: 0040 loss_train: 1.4195 acc_train: 0.6786 loss_val: 1.4885 acc_val: 0.6633 time: 0.0970s\n",
      "Epoch: 0041 loss_train: 1.3960 acc_train: 0.6500 loss_val: 1.4759 acc_val: 0.6700 time: 0.0930s\n",
      "Epoch: 0042 loss_train: 1.4275 acc_train: 0.5857 loss_val: 1.4634 acc_val: 0.6767 time: 0.0960s\n",
      "Epoch: 0043 loss_train: 1.3740 acc_train: 0.6500 loss_val: 1.4510 acc_val: 0.6867 time: 0.0940s\n",
      "Epoch: 0044 loss_train: 1.3974 acc_train: 0.6643 loss_val: 1.4387 acc_val: 0.6900 time: 0.0950s\n",
      "Epoch: 0045 loss_train: 1.2940 acc_train: 0.6500 loss_val: 1.4264 acc_val: 0.6967 time: 0.0926s\n",
      "Epoch: 0046 loss_train: 1.3823 acc_train: 0.6429 loss_val: 1.4143 acc_val: 0.7100 time: 0.0947s\n",
      "Epoch: 0047 loss_train: 1.3376 acc_train: 0.6929 loss_val: 1.4022 acc_val: 0.7133 time: 0.0913s\n",
      "Epoch: 0048 loss_train: 1.3084 acc_train: 0.6714 loss_val: 1.3903 acc_val: 0.7167 time: 0.0934s\n",
      "Epoch: 0049 loss_train: 1.3040 acc_train: 0.6857 loss_val: 1.3785 acc_val: 0.7200 time: 0.0925s\n",
      "Epoch: 0050 loss_train: 1.3682 acc_train: 0.6214 loss_val: 1.3671 acc_val: 0.7300 time: 0.0920s\n",
      "Epoch: 0051 loss_train: 1.3669 acc_train: 0.6286 loss_val: 1.3560 acc_val: 0.7333 time: 0.0943s\n",
      "Epoch: 0052 loss_train: 1.2445 acc_train: 0.6929 loss_val: 1.3451 acc_val: 0.7400 time: 0.0921s\n",
      "Epoch: 0053 loss_train: 1.3574 acc_train: 0.7143 loss_val: 1.3346 acc_val: 0.7433 time: 0.0947s\n",
      "Epoch: 0054 loss_train: 1.2639 acc_train: 0.6929 loss_val: 1.3243 acc_val: 0.7467 time: 0.0922s\n",
      "Epoch: 0055 loss_train: 1.2576 acc_train: 0.6714 loss_val: 1.3142 acc_val: 0.7533 time: 0.0949s\n",
      "Epoch: 0056 loss_train: 1.2499 acc_train: 0.6857 loss_val: 1.3044 acc_val: 0.7567 time: 0.0915s\n",
      "Epoch: 0057 loss_train: 1.2646 acc_train: 0.7000 loss_val: 1.2948 acc_val: 0.7700 time: 0.0925s\n",
      "Epoch: 0058 loss_train: 1.2123 acc_train: 0.7429 loss_val: 1.2854 acc_val: 0.7767 time: 0.0925s\n",
      "Epoch: 0059 loss_train: 1.2755 acc_train: 0.6929 loss_val: 1.2761 acc_val: 0.7833 time: 0.0925s\n",
      "Epoch: 0060 loss_train: 1.2751 acc_train: 0.6714 loss_val: 1.2671 acc_val: 0.7833 time: 0.0935s\n",
      "Epoch: 0061 loss_train: 1.1353 acc_train: 0.7500 loss_val: 1.2580 acc_val: 0.7933 time: 0.0919s\n",
      "Epoch: 0062 loss_train: 1.2755 acc_train: 0.7214 loss_val: 1.2492 acc_val: 0.7933 time: 0.0945s\n",
      "Epoch: 0063 loss_train: 1.1983 acc_train: 0.6786 loss_val: 1.2406 acc_val: 0.7933 time: 0.0915s\n",
      "Epoch: 0064 loss_train: 1.1510 acc_train: 0.7143 loss_val: 1.2320 acc_val: 0.7967 time: 0.0953s\n",
      "Epoch: 0065 loss_train: 1.1609 acc_train: 0.7286 loss_val: 1.2235 acc_val: 0.8033 time: 0.0915s\n",
      "Epoch: 0066 loss_train: 1.2087 acc_train: 0.7429 loss_val: 1.2152 acc_val: 0.8067 time: 0.0915s\n",
      "Epoch: 0067 loss_train: 1.1569 acc_train: 0.7357 loss_val: 1.2068 acc_val: 0.8100 time: 0.0920s\n",
      "Epoch: 0068 loss_train: 1.1902 acc_train: 0.7214 loss_val: 1.1987 acc_val: 0.8100 time: 0.0905s\n",
      "Epoch: 0069 loss_train: 1.1669 acc_train: 0.7786 loss_val: 1.1909 acc_val: 0.8167 time: 0.0943s\n",
      "Epoch: 0070 loss_train: 1.1193 acc_train: 0.7286 loss_val: 1.1831 acc_val: 0.8200 time: 0.0921s\n",
      "Epoch: 0071 loss_train: 1.1338 acc_train: 0.7571 loss_val: 1.1752 acc_val: 0.8300 time: 0.0936s\n",
      "Epoch: 0072 loss_train: 1.1351 acc_train: 0.7286 loss_val: 1.1674 acc_val: 0.8333 time: 0.0912s\n",
      "Epoch: 0073 loss_train: 1.0768 acc_train: 0.7214 loss_val: 1.1596 acc_val: 0.8333 time: 0.0945s\n",
      "Epoch: 0074 loss_train: 1.0528 acc_train: 0.7214 loss_val: 1.1520 acc_val: 0.8333 time: 0.0917s\n",
      "Epoch: 0075 loss_train: 0.9831 acc_train: 0.8000 loss_val: 1.1441 acc_val: 0.8333 time: 0.0922s\n",
      "Epoch: 0076 loss_train: 1.1571 acc_train: 0.6929 loss_val: 1.1366 acc_val: 0.8333 time: 0.0942s\n",
      "Epoch: 0077 loss_train: 0.9944 acc_train: 0.8286 loss_val: 1.1293 acc_val: 0.8333 time: 0.0916s\n",
      "Epoch: 0078 loss_train: 1.0944 acc_train: 0.7857 loss_val: 1.1220 acc_val: 0.8333 time: 0.0938s\n",
      "Epoch: 0079 loss_train: 1.1251 acc_train: 0.7714 loss_val: 1.1147 acc_val: 0.8367 time: 0.0915s\n",
      "Epoch: 0080 loss_train: 1.1038 acc_train: 0.7357 loss_val: 1.1076 acc_val: 0.8400 time: 0.0941s\n",
      "Epoch: 0081 loss_train: 1.0109 acc_train: 0.7786 loss_val: 1.1006 acc_val: 0.8400 time: 0.0905s\n",
      "Epoch: 0082 loss_train: 0.9571 acc_train: 0.7714 loss_val: 1.0937 acc_val: 0.8400 time: 0.0915s\n",
      "Epoch: 0083 loss_train: 1.0634 acc_train: 0.7500 loss_val: 1.0869 acc_val: 0.8400 time: 0.0921s\n",
      "Epoch: 0084 loss_train: 0.9854 acc_train: 0.8143 loss_val: 1.0801 acc_val: 0.8400 time: 0.0925s\n",
      "Epoch: 0085 loss_train: 1.0638 acc_train: 0.7714 loss_val: 1.0731 acc_val: 0.8400 time: 0.0937s\n",
      "Epoch: 0086 loss_train: 1.1149 acc_train: 0.7571 loss_val: 1.0662 acc_val: 0.8400 time: 0.0912s\n",
      "Epoch: 0087 loss_train: 1.0581 acc_train: 0.7500 loss_val: 1.0596 acc_val: 0.8400 time: 0.0952s\n",
      "Epoch: 0088 loss_train: 0.9987 acc_train: 0.7286 loss_val: 1.0532 acc_val: 0.8367 time: 0.0922s\n",
      "Epoch: 0089 loss_train: 1.0166 acc_train: 0.7429 loss_val: 1.0472 acc_val: 0.8400 time: 0.0920s\n",
      "Epoch: 0090 loss_train: 0.9474 acc_train: 0.8000 loss_val: 1.0410 acc_val: 0.8400 time: 0.0924s\n",
      "Epoch: 0091 loss_train: 1.0567 acc_train: 0.7714 loss_val: 1.0348 acc_val: 0.8400 time: 0.0920s\n",
      "Epoch: 0092 loss_train: 1.0484 acc_train: 0.7357 loss_val: 1.0289 acc_val: 0.8400 time: 0.0940s\n",
      "Epoch: 0093 loss_train: 0.9219 acc_train: 0.8000 loss_val: 1.0231 acc_val: 0.8433 time: 0.0911s\n",
      "Epoch: 0094 loss_train: 0.9459 acc_train: 0.7286 loss_val: 1.0172 acc_val: 0.8433 time: 0.0945s\n",
      "Epoch: 0095 loss_train: 0.9273 acc_train: 0.7643 loss_val: 1.0114 acc_val: 0.8433 time: 0.0911s\n",
      "Epoch: 0096 loss_train: 0.9915 acc_train: 0.7357 loss_val: 1.0057 acc_val: 0.8433 time: 0.0927s\n",
      "Epoch: 0097 loss_train: 0.9318 acc_train: 0.7643 loss_val: 1.0003 acc_val: 0.8433 time: 0.0922s\n",
      "Epoch: 0098 loss_train: 0.9286 acc_train: 0.8000 loss_val: 0.9950 acc_val: 0.8433 time: 0.0915s\n",
      "Epoch: 0099 loss_train: 0.9140 acc_train: 0.7643 loss_val: 0.9898 acc_val: 0.8433 time: 0.0935s\n",
      "Epoch: 0100 loss_train: 0.9694 acc_train: 0.7571 loss_val: 0.9851 acc_val: 0.8433 time: 0.0930s\n",
      "Epoch: 0101 loss_train: 1.0551 acc_train: 0.7143 loss_val: 0.9806 acc_val: 0.8400 time: 0.0941s\n",
      "Epoch: 0102 loss_train: 0.9186 acc_train: 0.8214 loss_val: 0.9765 acc_val: 0.8433 time: 0.0911s\n",
      "Epoch: 0103 loss_train: 0.8279 acc_train: 0.8000 loss_val: 0.9723 acc_val: 0.8467 time: 0.0946s\n",
      "Epoch: 0104 loss_train: 0.9027 acc_train: 0.8000 loss_val: 0.9684 acc_val: 0.8433 time: 0.0915s\n",
      "Epoch: 0105 loss_train: 0.9650 acc_train: 0.7286 loss_val: 0.9647 acc_val: 0.8467 time: 0.0920s\n",
      "Epoch: 0106 loss_train: 0.9523 acc_train: 0.7429 loss_val: 0.9612 acc_val: 0.8467 time: 0.0940s\n",
      "Epoch: 0107 loss_train: 0.8687 acc_train: 0.8214 loss_val: 0.9576 acc_val: 0.8400 time: 0.0911s\n",
      "Epoch: 0108 loss_train: 1.0035 acc_train: 0.7500 loss_val: 0.9542 acc_val: 0.8400 time: 0.0942s\n",
      "Epoch: 0109 loss_train: 0.8950 acc_train: 0.8000 loss_val: 0.9509 acc_val: 0.8367 time: 0.0921s\n",
      "Epoch: 0110 loss_train: 0.8288 acc_train: 0.7857 loss_val: 0.9474 acc_val: 0.8333 time: 0.0938s\n",
      "Epoch: 0111 loss_train: 0.8993 acc_train: 0.8143 loss_val: 0.9439 acc_val: 0.8333 time: 0.0921s\n",
      "Epoch: 0112 loss_train: 0.8710 acc_train: 0.7571 loss_val: 0.9404 acc_val: 0.8333 time: 0.0921s\n",
      "Epoch: 0113 loss_train: 0.9141 acc_train: 0.8071 loss_val: 0.9370 acc_val: 0.8300 time: 0.0915s\n",
      "Epoch: 0114 loss_train: 0.9235 acc_train: 0.7714 loss_val: 0.9336 acc_val: 0.8300 time: 0.0920s\n",
      "Epoch: 0115 loss_train: 0.8995 acc_train: 0.7571 loss_val: 0.9302 acc_val: 0.8300 time: 0.0941s\n",
      "Epoch: 0116 loss_train: 0.8733 acc_train: 0.7714 loss_val: 0.9269 acc_val: 0.8300 time: 0.0912s\n",
      "Epoch: 0117 loss_train: 0.8540 acc_train: 0.8214 loss_val: 0.9238 acc_val: 0.8333 time: 0.0945s\n",
      "Epoch: 0118 loss_train: 0.9211 acc_train: 0.8000 loss_val: 0.9206 acc_val: 0.8300 time: 0.0915s\n",
      "Epoch: 0119 loss_train: 0.8249 acc_train: 0.7714 loss_val: 0.9171 acc_val: 0.8300 time: 0.0912s\n",
      "Epoch: 0120 loss_train: 0.8585 acc_train: 0.8071 loss_val: 0.9137 acc_val: 0.8300 time: 0.0933s\n",
      "Epoch: 0121 loss_train: 0.8075 acc_train: 0.8071 loss_val: 0.9105 acc_val: 0.8300 time: 0.0913s\n",
      "Epoch: 0122 loss_train: 0.8591 acc_train: 0.7857 loss_val: 0.9071 acc_val: 0.8267 time: 0.0938s\n",
      "Epoch: 0123 loss_train: 0.9152 acc_train: 0.7429 loss_val: 0.9037 acc_val: 0.8267 time: 0.0915s\n",
      "Epoch: 0124 loss_train: 0.8111 acc_train: 0.8000 loss_val: 0.9003 acc_val: 0.8267 time: 0.0953s\n",
      "Epoch: 0125 loss_train: 0.7962 acc_train: 0.8143 loss_val: 0.8967 acc_val: 0.8267 time: 0.0915s\n",
      "Epoch: 0126 loss_train: 1.0218 acc_train: 0.7357 loss_val: 0.8938 acc_val: 0.8267 time: 0.0905s\n",
      "Epoch: 0127 loss_train: 0.9191 acc_train: 0.7571 loss_val: 0.8911 acc_val: 0.8267 time: 0.0922s\n",
      "Epoch: 0128 loss_train: 0.7758 acc_train: 0.8429 loss_val: 0.8884 acc_val: 0.8267 time: 0.0911s\n",
      "Epoch: 0129 loss_train: 0.9684 acc_train: 0.7000 loss_val: 0.8856 acc_val: 0.8267 time: 0.0943s\n",
      "Epoch: 0130 loss_train: 0.8091 acc_train: 0.8286 loss_val: 0.8827 acc_val: 0.8300 time: 0.0916s\n",
      "Epoch: 0131 loss_train: 0.7926 acc_train: 0.8571 loss_val: 0.8797 acc_val: 0.8300 time: 0.0942s\n",
      "Epoch: 0132 loss_train: 0.7829 acc_train: 0.8357 loss_val: 0.8770 acc_val: 0.8300 time: 0.0921s\n",
      "Epoch: 0133 loss_train: 0.8674 acc_train: 0.7857 loss_val: 0.8744 acc_val: 0.8300 time: 0.0912s\n",
      "Epoch: 0134 loss_train: 0.8842 acc_train: 0.7571 loss_val: 0.8717 acc_val: 0.8267 time: 0.0910s\n",
      "Epoch: 0135 loss_train: 0.8448 acc_train: 0.7571 loss_val: 0.8690 acc_val: 0.8233 time: 0.0914s\n",
      "Epoch: 0136 loss_train: 0.8166 acc_train: 0.8000 loss_val: 0.8663 acc_val: 0.8233 time: 0.0945s\n",
      "Epoch: 0137 loss_train: 0.8223 acc_train: 0.8071 loss_val: 0.8635 acc_val: 0.8233 time: 0.0912s\n",
      "Epoch: 0138 loss_train: 0.7794 acc_train: 0.8571 loss_val: 0.8610 acc_val: 0.8233 time: 0.0937s\n",
      "Epoch: 0139 loss_train: 0.8960 acc_train: 0.7286 loss_val: 0.8585 acc_val: 0.8267 time: 0.0915s\n",
      "Epoch: 0140 loss_train: 0.8388 acc_train: 0.8071 loss_val: 0.8560 acc_val: 0.8267 time: 0.0911s\n",
      "Epoch: 0141 loss_train: 0.8680 acc_train: 0.7571 loss_val: 0.8535 acc_val: 0.8267 time: 0.0942s\n",
      "Epoch: 0142 loss_train: 0.9153 acc_train: 0.7643 loss_val: 0.8511 acc_val: 0.8267 time: 0.0922s\n",
      "Epoch: 0143 loss_train: 0.7994 acc_train: 0.8143 loss_val: 0.8486 acc_val: 0.8267 time: 0.0947s\n",
      "Epoch: 0144 loss_train: 0.8114 acc_train: 0.7500 loss_val: 0.8461 acc_val: 0.8300 time: 0.0925s\n",
      "Epoch: 0145 loss_train: 0.7955 acc_train: 0.8143 loss_val: 0.8438 acc_val: 0.8300 time: 0.0940s\n",
      "Epoch: 0146 loss_train: 0.9080 acc_train: 0.7643 loss_val: 0.8414 acc_val: 0.8367 time: 0.0915s\n",
      "Epoch: 0147 loss_train: 0.8417 acc_train: 0.7714 loss_val: 0.8390 acc_val: 0.8367 time: 0.0934s\n",
      "Epoch: 0148 loss_train: 0.8347 acc_train: 0.8000 loss_val: 0.8369 acc_val: 0.8367 time: 0.0916s\n",
      "Epoch: 0149 loss_train: 0.8532 acc_train: 0.7929 loss_val: 0.8349 acc_val: 0.8367 time: 0.0915s\n",
      "Epoch: 0150 loss_train: 0.8017 acc_train: 0.8214 loss_val: 0.8328 acc_val: 0.8367 time: 0.0945s\n",
      "Epoch: 0151 loss_train: 0.7017 acc_train: 0.8143 loss_val: 0.8309 acc_val: 0.8333 time: 0.0910s\n",
      "Epoch: 0152 loss_train: 0.7967 acc_train: 0.7857 loss_val: 0.8291 acc_val: 0.8333 time: 0.0945s\n",
      "Epoch: 0153 loss_train: 0.7516 acc_train: 0.8214 loss_val: 0.8271 acc_val: 0.8333 time: 0.0917s\n",
      "Epoch: 0154 loss_train: 0.8526 acc_train: 0.7714 loss_val: 0.8250 acc_val: 0.8333 time: 0.0945s\n",
      "Epoch: 0155 loss_train: 0.8873 acc_train: 0.7429 loss_val: 0.8230 acc_val: 0.8333 time: 0.0916s\n",
      "Epoch: 0156 loss_train: 0.8119 acc_train: 0.7643 loss_val: 0.8210 acc_val: 0.8333 time: 0.0922s\n",
      "Epoch: 0157 loss_train: 0.8037 acc_train: 0.8000 loss_val: 0.8190 acc_val: 0.8333 time: 0.0920s\n",
      "Epoch: 0158 loss_train: 0.7131 acc_train: 0.8143 loss_val: 0.8171 acc_val: 0.8333 time: 0.0905s\n",
      "Epoch: 0159 loss_train: 0.7819 acc_train: 0.7857 loss_val: 0.8152 acc_val: 0.8333 time: 0.0925s\n",
      "Epoch: 0160 loss_train: 0.7917 acc_train: 0.7500 loss_val: 0.8133 acc_val: 0.8333 time: 0.0913s\n",
      "Epoch: 0161 loss_train: 0.8251 acc_train: 0.8000 loss_val: 0.8116 acc_val: 0.8333 time: 0.0933s\n",
      "Epoch: 0162 loss_train: 0.8765 acc_train: 0.7643 loss_val: 0.8100 acc_val: 0.8333 time: 0.0905s\n",
      "Epoch: 0163 loss_train: 0.7697 acc_train: 0.8143 loss_val: 0.8085 acc_val: 0.8333 time: 0.0905s\n",
      "Epoch: 0164 loss_train: 0.8333 acc_train: 0.7857 loss_val: 0.8071 acc_val: 0.8333 time: 0.0934s\n",
      "Epoch: 0165 loss_train: 0.7359 acc_train: 0.7714 loss_val: 0.8056 acc_val: 0.8367 time: 0.0910s\n",
      "Epoch: 0166 loss_train: 0.7613 acc_train: 0.8143 loss_val: 0.8042 acc_val: 0.8367 time: 0.0935s\n",
      "Epoch: 0167 loss_train: 0.8393 acc_train: 0.8071 loss_val: 0.8028 acc_val: 0.8300 time: 0.0907s\n",
      "Epoch: 0168 loss_train: 0.7574 acc_train: 0.8071 loss_val: 0.8015 acc_val: 0.8300 time: 0.0941s\n",
      "Epoch: 0169 loss_train: 0.7914 acc_train: 0.7929 loss_val: 0.8000 acc_val: 0.8300 time: 0.0912s\n",
      "Epoch: 0170 loss_train: 0.8264 acc_train: 0.7714 loss_val: 0.7988 acc_val: 0.8300 time: 0.0911s\n",
      "Epoch: 0171 loss_train: 0.7705 acc_train: 0.7643 loss_val: 0.7978 acc_val: 0.8267 time: 0.0943s\n",
      "Epoch: 0172 loss_train: 0.9153 acc_train: 0.7714 loss_val: 0.7969 acc_val: 0.8267 time: 0.0907s\n",
      "Epoch: 0173 loss_train: 0.8228 acc_train: 0.8000 loss_val: 0.7960 acc_val: 0.8267 time: 0.0931s\n",
      "Epoch: 0174 loss_train: 0.8757 acc_train: 0.7714 loss_val: 0.7951 acc_val: 0.8233 time: 0.0905s\n",
      "Epoch: 0175 loss_train: 0.7946 acc_train: 0.7857 loss_val: 0.7943 acc_val: 0.8233 time: 0.0935s\n",
      "Epoch: 0176 loss_train: 0.7813 acc_train: 0.7857 loss_val: 0.7935 acc_val: 0.8233 time: 0.0905s\n",
      "Epoch: 0177 loss_train: 0.7674 acc_train: 0.8143 loss_val: 0.7927 acc_val: 0.8233 time: 0.0905s\n",
      "Epoch: 0178 loss_train: 0.7459 acc_train: 0.8071 loss_val: 0.7918 acc_val: 0.8233 time: 0.0929s\n",
      "Epoch: 0179 loss_train: 0.8247 acc_train: 0.7786 loss_val: 0.7906 acc_val: 0.8233 time: 0.0911s\n",
      "Epoch: 0180 loss_train: 0.8874 acc_train: 0.7286 loss_val: 0.7896 acc_val: 0.8233 time: 0.0941s\n",
      "Epoch: 0181 loss_train: 0.8330 acc_train: 0.7500 loss_val: 0.7885 acc_val: 0.8233 time: 0.0915s\n",
      "Epoch: 0182 loss_train: 0.7946 acc_train: 0.7857 loss_val: 0.7874 acc_val: 0.8233 time: 0.0907s\n",
      "Epoch: 0183 loss_train: 0.7173 acc_train: 0.7929 loss_val: 0.7863 acc_val: 0.8233 time: 0.0936s\n",
      "Epoch: 0184 loss_train: 0.7657 acc_train: 0.7714 loss_val: 0.7852 acc_val: 0.8300 time: 0.0914s\n",
      "Epoch: 0185 loss_train: 0.6405 acc_train: 0.8357 loss_val: 0.7840 acc_val: 0.8333 time: 0.0945s\n",
      "Epoch: 0186 loss_train: 0.6819 acc_train: 0.8286 loss_val: 0.7825 acc_val: 0.8333 time: 0.0905s\n",
      "Epoch: 0187 loss_train: 0.7474 acc_train: 0.8286 loss_val: 0.7808 acc_val: 0.8333 time: 0.0926s\n",
      "Epoch: 0188 loss_train: 0.7485 acc_train: 0.8143 loss_val: 0.7792 acc_val: 0.8333 time: 0.0911s\n",
      "Epoch: 0189 loss_train: 0.7287 acc_train: 0.8214 loss_val: 0.7777 acc_val: 0.8333 time: 0.0915s\n",
      "Epoch: 0190 loss_train: 0.8120 acc_train: 0.7714 loss_val: 0.7764 acc_val: 0.8333 time: 0.0938s\n",
      "Epoch: 0191 loss_train: 0.8387 acc_train: 0.7500 loss_val: 0.7753 acc_val: 0.8333 time: 0.0901s\n",
      "Epoch: 0192 loss_train: 0.8783 acc_train: 0.7714 loss_val: 0.7744 acc_val: 0.8333 time: 0.0931s\n",
      "Epoch: 0193 loss_train: 0.7951 acc_train: 0.7429 loss_val: 0.7736 acc_val: 0.8333 time: 0.0911s\n",
      "Epoch: 0194 loss_train: 0.8314 acc_train: 0.7643 loss_val: 0.7727 acc_val: 0.8333 time: 0.0931s\n",
      "Epoch: 0195 loss_train: 0.7922 acc_train: 0.7929 loss_val: 0.7720 acc_val: 0.8333 time: 0.0912s\n",
      "Epoch: 0196 loss_train: 0.6718 acc_train: 0.8357 loss_val: 0.7713 acc_val: 0.8333 time: 0.0920s\n",
      "Epoch: 0197 loss_train: 0.8063 acc_train: 0.7500 loss_val: 0.7705 acc_val: 0.8333 time: 0.0927s\n",
      "Epoch: 0198 loss_train: 0.7731 acc_train: 0.7643 loss_val: 0.7697 acc_val: 0.8333 time: 0.0905s\n",
      "Epoch: 0199 loss_train: 0.7972 acc_train: 0.8143 loss_val: 0.7686 acc_val: 0.8333 time: 0.0932s\n",
      "Epoch: 0200 loss_train: 0.8144 acc_train: 0.7643 loss_val: 0.7676 acc_val: 0.8333 time: 0.0915s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 19.1413s\n",
      "Test set results: loss= 0.8177 accuracy= 0.8380\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = param[\"epochs\"] + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(param[\"epochs\"]):\n",
    "    loss_values.append(train(epoch))\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    # 如果验证集的损失值小于最好的损失值，就更新最好的损失值\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "    # 如果连续bad_counter次验证集的损失值都没有更新，就停止训练\n",
    "    if bad_counter == param[\"patience\"]:\n",
    "        break\n",
    "\n",
    "    file = glob.glob('*.pkl')\n",
    "    for f in file:\n",
    "        epoch_nb = int(f.split('.')[0])\n",
    "        if epoch_nb < best_epoch: # 每个epoch都只会保留最好的模型，把在最好模型之前的模型都删除\n",
    "            os.remove(f)\n",
    "    \n",
    "files = glob.glob('*.pkl')\n",
    "for f in files:\n",
    "    epoch_nb = int(f.split('.')[0])\n",
    "    if epoch_nb > best_epoch: # 训练结束以后删除epoch在最好模型之后的模型，因为这些模型都是无用的\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# 加载最好的模型\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "test() # 测试模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
