{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GAT 代码复现</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora_content:\n",
      "       0     1     2     3     4     5     6     7     8     9     ...  1425  \\\n",
      "0    31336     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1  1061127     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2  1106406     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3    13195     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4    37879     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1426  1427  1428  1429  1430  1431  1432  1433                    1434  \n",
      "0     0     1     0     0     0     0     0     0         Neural_Networks  \n",
      "1     1     0     0     0     0     0     0     0           Rule_Learning  \n",
      "2     0     0     0     0     0     0     0     0  Reinforcement_Learning  \n",
      "3     0     0     0     0     0     0     0     0  Reinforcement_Learning  \n",
      "4     0     0     0     0     0     0     0     0   Probabilistic_Methods  \n",
      "\n",
      "[5 rows x 1435 columns]\n",
      "cora_cites:\n",
      "     0        1\n",
      "0  35     1033\n",
      "1  35   103482\n",
      "2  35   103515\n",
      "3  35  1050679\n",
      "4  35  1103960\n"
     ]
    }
   ],
   "source": [
    "# 数据导入\n",
    "cora_content_path = '../dataset/cora/cora.content'\n",
    "cora_content = pd.read_csv(cora_content_path, sep='\\t', header=None)\n",
    "cora_cites_path = '../dataset/cora/cora.cites'\n",
    "cora_cites = pd.read_csv(cora_cites_path, sep='\\t', header=None)\n",
    "\n",
    "# 打印数据的前5行\n",
    "print(\"cora_content:\\n\",cora_content.head())\n",
    "print(\"cora_cites:\\n\",cora_cites.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义辅助函数热独编码,这里的labels是一个列表\n",
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels))) # set去重，sorted按照从小到大排序\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)} # 生成热独编码字典 {类别：热独编码}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32) # map函数，对labels中的每个元素进行热独编码,get函数单独使用，返回字典中key对应的value\n",
    "    return labels_onehot # 返回热独编码后的标签,shape为(N,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数据归一化,输入维度为(N,1433)\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)) # 按行求和，shape为(N,1)\n",
    "    r_inv = np.power(rowsum, -1).flatten() # 求倒数,flatten()将多维数组降为一维\n",
    "    r_inv[np.isinf(r_inv)] = 0. # 将无穷大的值设置为0\n",
    "    r_mat_inv = sp.diags(r_inv) # 对角矩阵\n",
    "    mx = r_mat_inv.dot(mx) # 矩阵相乘，实现归一化，shape为(N,F)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邻接矩阵数据归一化\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)) # 按行求和，shape为(N,1)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten() # 求倒数的平方根\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0. # 将无穷大的值设置为0\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt) # 对角矩阵\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt) # 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确率，输出的output的形状是(N,C),labels的形状是(N,)，其中N是样本数，C是类别数\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels) # 返回每行最大值的索引，即预测的类别\n",
    "    correct = preds.eq(labels).double() # 判断是否相等\n",
    "    correct = correct.sum() # 统计相等的个数\n",
    "    return correct / len(labels) # 返回准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征以及标签读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "idx_features_labels = np.genfromtxt(cora_content_path, dtype=np.dtype(str)) # 读取数据\n",
    "# 从数据中提取特征\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # 提取特征\n",
    "# 提取标签\n",
    "labels = encode_onehot(idx_features_labels[:, -1]) # 提取标签,-1表示最后一列，同时进行热独编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成图状结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成图结构\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # 提取索引\n",
    "idx_map = {j: i for i, j in enumerate(idx)} # 生成索引字典,{原索引：新索引},enumerate将列表转换为索引-值对\n",
    "# 生成边信息\n",
    "edges_unordered = np.genfromtxt(cora_cites_path, dtype=np.int32) # 读取边信息，shape=(5429,2)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape) # 生成重新编号的边信息，shape=(5429,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32) # 生成邻接矩阵的稀疏矩阵，np.ones(edges.shape[0])表示矩阵中所有非零元素的值，edges[:, 0]和edges[:, 1]分别表示这些非零元素的行索引和列索引。\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) # 对称归一化邻接矩阵,因为无向图，所以邻接矩阵应该是对称的，这段代码将adj更新为adj和adj.T中的较大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = normalize_features(features) # 特征归一化\n",
    "adj = normalize_adj(adj + sp.eye(adj.shape[0])) # 邻接矩阵归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分成训练集、验证集和测试集\n",
    "idx_train = range(140) # 训练集\n",
    "idx_val = range(200, 500) # 验证集\n",
    "idx_test = range(500, 1500) # 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据转换成能导入模型的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.FloatTensor(np.array(adj.todense())) # 将邻接矩阵转换为tensor\n",
    "features = torch.FloatTensor(np.array(features.todense())) # 将特征转换为tensor\n",
    "labels = torch.LongTensor(np.where(labels)[1]) # 将标签转换为tensor\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train) # 将训练集索引转换为tensor\n",
    "idx_val = torch.LongTensor(idx_val) # 将验证集索引转换为tensor\n",
    "idx_test = torch.LongTensor(idx_test) # 将测试集索引转换为tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步计算：\n",
    "$e_{ij}=a(\\mathbf{W}\\vec{h}_i,\\mathbf{W}\\vec{h}_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步计算：\n",
    "$\\alpha_{ij}=\\mathrm{softmax}_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\exp(e_{ik})}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合计算公式：\n",
    "$\\alpha_{ij}=\\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\mathbf{a}}^T[\\mathbf{W}\\vec{h}_i\\|\\mathbf{W}\\vec{h}_j]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i}\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\mathbf{a}}^T[\\mathbf{W}\\vec{h}_i\\|\\mathbf{W}\\vec{h}_k]\\right)\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h'计算公式：\n",
    "$\\vec{h}_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}\\vec{h}_j\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSingleLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat = True): # in_features是输入特征维度，out_features是输出特征维度，dropout是dropout的概率，alpha是leakyrelu的参数，concat是否要拼接\n",
    "        super(GSingleLayer, self).__init__()\n",
    "        self.dropout = dropout # 在数据量较少的时候，可以使用dropout防止过拟合\n",
    "        self.in_features = in_features # 输入特征维度F\n",
    "        self.out_features = out_features # 输出特征维度F'\n",
    "        self.concat = concat # 是否要拼接，对于中间层，需要拼接因为这里使用的是多头注意力，但是对于最后一层就不需要拼接直接计算平均值\n",
    "        self.alpha = alpha # leakyrelu的参数\n",
    "\n",
    "        # 定义权重矩阵\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))) # 定义权重矩阵,shape=[F,F'],这里跟论文中的不一样是后面在将数据导入模型的时候纬度是[N,F]。这里要相应进行调整\n",
    "        # 用Xavier方法初始化权重矩阵\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414) # 使用Xavier方法初始化权重矩阵\n",
    "\n",
    "        # 定义权重向量a\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1))) # 定义权重向量a,shape=[2F',1]\n",
    "        # 用Xavier方法初始化权重向量a\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    # 定义在这一层节点会进行什么操作\n",
    "    # h就是论文中的输入特征集合，adj就是邻接矩阵\n",
    "    def forward(self, h, adj):\n",
    "        # 首先是e的计算，矩阵的使用用高维的避免了循环遍历\n",
    "        Wh = torch.mm(h, self.W) # shape [N, F']\n",
    "        Wh1 = torch.matmul(Wh,self.a[:self.out_features, :]) # shape [N, 1], 这里将a拆开相当于后面将两个拼接起来\n",
    "        Wh2 = torch.matmul(Wh,self.a[self.out_features:, :]) # shape [N, 1]，这里一定要记住不能少了后面这个冒号！！！！！\n",
    "        e = self.leakyrelu(Wh1 + Wh2.T) # shape [N, N]，同时这里使用了广播相加\n",
    "\n",
    "        # 计算注意力系数a\n",
    "        zero_vec = -9e15*torch.ones_like(e) # 生成负无穷矩阵，方便后续经过softmax函数直接变成0\n",
    "        attention = torch.where(adj > 0, e, zero_vec) # 前面计算了所有节点之间的相关性系数，这里筛选出有边关系的保留attention,剩下的不保留\n",
    "        attention = F.softmax(attention, dim = 1)\n",
    "        attention = F.dropout(attention, self.dropout, training = self.training)\n",
    "        h_prime = torch.matmul(attention, Wh) # 将所有的attention和Wh相乘得到当前层的输出\n",
    "\n",
    "        if self.concat: # 如果是拼接的话\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Network(nn.Module):\n",
    "    def __init__(self, nfeature, nhidden, nclass, dropout, alpha, nheads): # nfeature是输入特征的维度，nhidden是隐藏层的维度，nclass是输出的类别数，dropout是dropout的概率，alpha是leakyrelu的参数，nheads是多头注意力的头数\n",
    "        super(GAT_Network, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads \n",
    "        # 定义多个注意力层\n",
    "        self.attentions = [GSingleLayer(nfeature, nhidden, dropout, alpha) for _ in range(nheads)] # 定义多个注意力层，输出是[nhidden, nheads]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            # 将多个注意力层加入到模型中，并行计算\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        # 定义输出层，输出层的输入维度就是上一层[nhidden, nheads]，由于这里是最后一层，根据论文中说的就不进行拼接了\n",
    "        self.out_att = GSingleLayer(nhidden * nheads, nclass, dropout, alpha, concat = False)\n",
    "    \n",
    "    # 定义在网络架构中信息的传递规则\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training = self.training) # dropout\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim = 1) # 将多个注意力层的输出拼接起来\n",
    "        x = F.dropout(x, self.dropout, training = self.training)\n",
    "        x = F.elu(self.out_att(x, adj)) # 用elu激活函数\n",
    "        return F.log_softmax(x, dim = 1) # softmax将输出转换为概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练以及评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子,保证实验的可重复性\n",
    "seed = 72\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# 定义训练参数序列\n",
    "param = {\n",
    "\n",
    "    \"epochs\": 200, # \"Number of epochs to train.\"\n",
    "    'lr': 0.005,  # \"Initial learning rate.\"\n",
    "    'weight_decay': 5e-4,  # \"Weight decay (L2 loss on parameters).\"\n",
    "    'hidden': 8,  # \"Number of hidden units.\"\n",
    "    'nb_heads': 8,  # \"Number of head attentions.\"\n",
    "    'dropout': 0.6,  # \"Dropout rate (1 - keep probability).\"\n",
    "    'alpha': 0.2,  # \"Alpha for the leaky_relu.\"\n",
    "    'patience': 100,    # \"Patience\"\n",
    "    'fastmode': False  # \"Activate fast mode (i.e. validate during training).\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transductive = GAT_Network(\n",
    "    nfeature = features.shape[1],\n",
    "    nhidden = param[\"hidden\"],\n",
    "    nclass = labels.max().item() + 1,\n",
    "    dropout = param[\"dropout\"],\n",
    "    alpha = param[\"alpha\"],\n",
    "    nheads = param[\"nb_heads\"]\n",
    ")\n",
    "optimizer = optim.Adam(\n",
    "    model_transductive.parameters(), # 模型参数\n",
    "    lr = param[\"lr\"],\n",
    "    weight_decay = param[\"weight_decay\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将所有信息加载到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433]) torch.Size([2708, 2708]) torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_transductive.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "# 将数据转换为Variable,目的是为了计算梯度\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels) \n",
    "# 打印数据形状\n",
    "print(features.shape,adj.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # 训练模型\n",
    "    model_transductive.train() # 训练模式\n",
    "    optimizer.zero_grad() # 梯度清零\n",
    "    output = model_transductive(features, adj) # 前向传播\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) # 计算损失\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train]) # 计算准确率\n",
    "    loss_train.backward() # 反向传播\n",
    "    optimizer.step() # 更新参数\n",
    "\n",
    "    if not param[\"fastmode\"]: # 如果不是快速模式,验证集和测试集也要进行训练\n",
    "        model_transductive.eval()\n",
    "        output = model_transductive(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "        'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model_transductive.eval() # 测试模式\n",
    "    output = model_transductive(features, adj) # 前向传播\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) # 计算损失\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test]) # 计算准确率\n",
    "    print(\"Test set results for dataset {} in transductive way:\".format(\"cora\"),\n",
    "        \"loss= {:.4f}\".format(loss_test.item()),\n",
    "        \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9484 acc_train: 0.0929 loss_val: 1.9408 acc_val: 0.2833 time: 2.3772s\n",
      "Epoch: 0002 loss_train: 1.9371 acc_train: 0.2071 loss_val: 1.9315 acc_val: 0.5367 time: 0.1059s\n",
      "Epoch: 0003 loss_train: 1.9255 acc_train: 0.2786 loss_val: 1.9219 acc_val: 0.6467 time: 0.1049s\n",
      "Epoch: 0004 loss_train: 1.9129 acc_train: 0.4571 loss_val: 1.9122 acc_val: 0.6433 time: 0.1045s\n",
      "Epoch: 0005 loss_train: 1.9053 acc_train: 0.4429 loss_val: 1.9027 acc_val: 0.6400 time: 0.1109s\n",
      "Epoch: 0006 loss_train: 1.8876 acc_train: 0.5429 loss_val: 1.8933 acc_val: 0.6500 time: 0.1032s\n",
      "Epoch: 0007 loss_train: 1.8828 acc_train: 0.5500 loss_val: 1.8838 acc_val: 0.6500 time: 0.1059s\n",
      "Epoch: 0008 loss_train: 1.8656 acc_train: 0.5714 loss_val: 1.8740 acc_val: 0.6333 time: 0.1040s\n",
      "Epoch: 0009 loss_train: 1.8482 acc_train: 0.5500 loss_val: 1.8640 acc_val: 0.6267 time: 0.1077s\n",
      "Epoch: 0010 loss_train: 1.8381 acc_train: 0.5714 loss_val: 1.8539 acc_val: 0.6267 time: 0.1089s\n",
      "Epoch: 0011 loss_train: 1.8292 acc_train: 0.5429 loss_val: 1.8436 acc_val: 0.6200 time: 0.1041s\n",
      "Epoch: 0012 loss_train: 1.8222 acc_train: 0.5429 loss_val: 1.8331 acc_val: 0.6200 time: 0.1040s\n",
      "Epoch: 0013 loss_train: 1.8241 acc_train: 0.5500 loss_val: 1.8224 acc_val: 0.6133 time: 0.1003s\n",
      "Epoch: 0014 loss_train: 1.8051 acc_train: 0.5714 loss_val: 1.8116 acc_val: 0.6100 time: 0.1079s\n",
      "Epoch: 0015 loss_train: 1.8065 acc_train: 0.5357 loss_val: 1.8007 acc_val: 0.6033 time: 0.1050s\n",
      "Epoch: 0016 loss_train: 1.7682 acc_train: 0.5786 loss_val: 1.7895 acc_val: 0.6033 time: 0.1061s\n",
      "Epoch: 0017 loss_train: 1.7729 acc_train: 0.6071 loss_val: 1.7781 acc_val: 0.6100 time: 0.0962s\n",
      "Epoch: 0018 loss_train: 1.7533 acc_train: 0.5357 loss_val: 1.7666 acc_val: 0.6067 time: 0.1034s\n",
      "Epoch: 0019 loss_train: 1.7137 acc_train: 0.5786 loss_val: 1.7548 acc_val: 0.6067 time: 0.1035s\n",
      "Epoch: 0020 loss_train: 1.7092 acc_train: 0.5571 loss_val: 1.7428 acc_val: 0.6100 time: 0.1087s\n",
      "Epoch: 0021 loss_train: 1.6915 acc_train: 0.5857 loss_val: 1.7306 acc_val: 0.6100 time: 0.1068s\n",
      "Epoch: 0022 loss_train: 1.6650 acc_train: 0.6071 loss_val: 1.7182 acc_val: 0.6100 time: 0.0991s\n",
      "Epoch: 0023 loss_train: 1.6619 acc_train: 0.5786 loss_val: 1.7056 acc_val: 0.6200 time: 0.1050s\n",
      "Epoch: 0024 loss_train: 1.6518 acc_train: 0.5786 loss_val: 1.6930 acc_val: 0.6200 time: 0.1114s\n",
      "Epoch: 0025 loss_train: 1.6718 acc_train: 0.5786 loss_val: 1.6804 acc_val: 0.6300 time: 0.1010s\n",
      "Epoch: 0026 loss_train: 1.6161 acc_train: 0.6500 loss_val: 1.6676 acc_val: 0.6367 time: 0.1050s\n",
      "Epoch: 0027 loss_train: 1.6277 acc_train: 0.5500 loss_val: 1.6549 acc_val: 0.6367 time: 0.1020s\n",
      "Epoch: 0028 loss_train: 1.5759 acc_train: 0.5786 loss_val: 1.6420 acc_val: 0.6400 time: 0.1056s\n",
      "Epoch: 0029 loss_train: 1.5535 acc_train: 0.6000 loss_val: 1.6291 acc_val: 0.6400 time: 0.1076s\n",
      "Epoch: 0030 loss_train: 1.5886 acc_train: 0.5643 loss_val: 1.6162 acc_val: 0.6500 time: 0.1080s\n",
      "Epoch: 0031 loss_train: 1.5280 acc_train: 0.6071 loss_val: 1.6032 acc_val: 0.6500 time: 0.1037s\n",
      "Epoch: 0032 loss_train: 1.5334 acc_train: 0.6286 loss_val: 1.5902 acc_val: 0.6500 time: 0.1035s\n",
      "Epoch: 0033 loss_train: 1.5328 acc_train: 0.5643 loss_val: 1.5773 acc_val: 0.6500 time: 0.1021s\n",
      "Epoch: 0034 loss_train: 1.4740 acc_train: 0.7000 loss_val: 1.5643 acc_val: 0.6533 time: 0.1118s\n",
      "Epoch: 0035 loss_train: 1.5168 acc_train: 0.6357 loss_val: 1.5515 acc_val: 0.6567 time: 0.1075s\n",
      "Epoch: 0036 loss_train: 1.4830 acc_train: 0.6000 loss_val: 1.5387 acc_val: 0.6567 time: 0.1010s\n",
      "Epoch: 0037 loss_train: 1.4562 acc_train: 0.6357 loss_val: 1.5261 acc_val: 0.6600 time: 0.1049s\n",
      "Epoch: 0038 loss_train: 1.4578 acc_train: 0.6071 loss_val: 1.5135 acc_val: 0.6600 time: 0.1040s\n",
      "Epoch: 0039 loss_train: 1.4937 acc_train: 0.6000 loss_val: 1.5010 acc_val: 0.6633 time: 0.1022s\n",
      "Epoch: 0040 loss_train: 1.4195 acc_train: 0.6786 loss_val: 1.4885 acc_val: 0.6633 time: 0.1049s\n",
      "Epoch: 0041 loss_train: 1.3960 acc_train: 0.6500 loss_val: 1.4759 acc_val: 0.6700 time: 0.1099s\n",
      "Epoch: 0042 loss_train: 1.4275 acc_train: 0.5857 loss_val: 1.4634 acc_val: 0.6767 time: 0.1045s\n",
      "Epoch: 0043 loss_train: 1.3740 acc_train: 0.6500 loss_val: 1.4510 acc_val: 0.6867 time: 0.1050s\n",
      "Epoch: 0044 loss_train: 1.3974 acc_train: 0.6643 loss_val: 1.4387 acc_val: 0.6900 time: 0.1027s\n",
      "Epoch: 0045 loss_train: 1.2940 acc_train: 0.6500 loss_val: 1.4264 acc_val: 0.6967 time: 0.1058s\n",
      "Epoch: 0046 loss_train: 1.3823 acc_train: 0.6429 loss_val: 1.4143 acc_val: 0.7100 time: 0.1020s\n",
      "Epoch: 0047 loss_train: 1.3376 acc_train: 0.6929 loss_val: 1.4022 acc_val: 0.7133 time: 0.1020s\n",
      "Epoch: 0048 loss_train: 1.3084 acc_train: 0.6714 loss_val: 1.3903 acc_val: 0.7167 time: 0.1043s\n",
      "Epoch: 0049 loss_train: 1.3040 acc_train: 0.6857 loss_val: 1.3785 acc_val: 0.7200 time: 0.1080s\n",
      "Epoch: 0050 loss_train: 1.3682 acc_train: 0.6214 loss_val: 1.3671 acc_val: 0.7300 time: 0.1042s\n",
      "Epoch: 0051 loss_train: 1.3669 acc_train: 0.6286 loss_val: 1.3560 acc_val: 0.7333 time: 0.1040s\n",
      "Epoch: 0052 loss_train: 1.2445 acc_train: 0.6929 loss_val: 1.3451 acc_val: 0.7400 time: 0.0981s\n",
      "Epoch: 0053 loss_train: 1.3574 acc_train: 0.7143 loss_val: 1.3346 acc_val: 0.7433 time: 0.1129s\n",
      "Epoch: 0054 loss_train: 1.2639 acc_train: 0.6929 loss_val: 1.3243 acc_val: 0.7467 time: 0.1049s\n",
      "Epoch: 0055 loss_train: 1.2576 acc_train: 0.6714 loss_val: 1.3142 acc_val: 0.7533 time: 0.1025s\n",
      "Epoch: 0056 loss_train: 1.2499 acc_train: 0.6857 loss_val: 1.3044 acc_val: 0.7567 time: 0.1022s\n",
      "Epoch: 0057 loss_train: 1.2646 acc_train: 0.7000 loss_val: 1.2948 acc_val: 0.7700 time: 0.1034s\n",
      "Epoch: 0058 loss_train: 1.2123 acc_train: 0.7429 loss_val: 1.2854 acc_val: 0.7767 time: 0.1099s\n",
      "Epoch: 0059 loss_train: 1.2755 acc_train: 0.6929 loss_val: 1.2761 acc_val: 0.7833 time: 0.1040s\n",
      "Epoch: 0060 loss_train: 1.2751 acc_train: 0.6714 loss_val: 1.2671 acc_val: 0.7833 time: 0.1025s\n",
      "Epoch: 0061 loss_train: 1.1353 acc_train: 0.7500 loss_val: 1.2580 acc_val: 0.7933 time: 0.1045s\n",
      "Epoch: 0062 loss_train: 1.2755 acc_train: 0.7214 loss_val: 1.2492 acc_val: 0.7933 time: 0.1035s\n",
      "Epoch: 0063 loss_train: 1.1983 acc_train: 0.6786 loss_val: 1.2406 acc_val: 0.7933 time: 0.1050s\n",
      "Epoch: 0064 loss_train: 1.1510 acc_train: 0.7143 loss_val: 1.2320 acc_val: 0.7967 time: 0.1050s\n",
      "Epoch: 0065 loss_train: 1.1609 acc_train: 0.7286 loss_val: 1.2235 acc_val: 0.8033 time: 0.1077s\n",
      "Epoch: 0066 loss_train: 1.2087 acc_train: 0.7429 loss_val: 1.2152 acc_val: 0.8067 time: 0.1020s\n",
      "Epoch: 0067 loss_train: 1.1569 acc_train: 0.7357 loss_val: 1.2068 acc_val: 0.8100 time: 0.0939s\n",
      "Epoch: 0068 loss_train: 1.1902 acc_train: 0.7214 loss_val: 1.1987 acc_val: 0.8100 time: 0.0958s\n",
      "Epoch: 0069 loss_train: 1.1669 acc_train: 0.7786 loss_val: 1.1909 acc_val: 0.8167 time: 0.0938s\n",
      "Epoch: 0070 loss_train: 1.1193 acc_train: 0.7286 loss_val: 1.1831 acc_val: 0.8200 time: 0.0981s\n",
      "Epoch: 0071 loss_train: 1.1338 acc_train: 0.7571 loss_val: 1.1752 acc_val: 0.8300 time: 0.1040s\n",
      "Epoch: 0072 loss_train: 1.1351 acc_train: 0.7286 loss_val: 1.1674 acc_val: 0.8333 time: 0.1039s\n",
      "Epoch: 0073 loss_train: 1.0768 acc_train: 0.7214 loss_val: 1.1596 acc_val: 0.8333 time: 0.1040s\n",
      "Epoch: 0074 loss_train: 1.0528 acc_train: 0.7214 loss_val: 1.1520 acc_val: 0.8333 time: 0.1020s\n",
      "Epoch: 0075 loss_train: 0.9831 acc_train: 0.8000 loss_val: 1.1441 acc_val: 0.8333 time: 0.1050s\n",
      "Epoch: 0076 loss_train: 1.1571 acc_train: 0.6929 loss_val: 1.1366 acc_val: 0.8333 time: 0.1049s\n",
      "Epoch: 0077 loss_train: 0.9944 acc_train: 0.8286 loss_val: 1.1293 acc_val: 0.8333 time: 0.1026s\n",
      "Epoch: 0078 loss_train: 1.0944 acc_train: 0.7857 loss_val: 1.1220 acc_val: 0.8333 time: 0.1054s\n",
      "Epoch: 0079 loss_train: 1.1251 acc_train: 0.7714 loss_val: 1.1147 acc_val: 0.8367 time: 0.1000s\n",
      "Epoch: 0080 loss_train: 1.1038 acc_train: 0.7357 loss_val: 1.1076 acc_val: 0.8400 time: 0.1099s\n",
      "Epoch: 0081 loss_train: 1.0109 acc_train: 0.7786 loss_val: 1.1006 acc_val: 0.8400 time: 0.1060s\n",
      "Epoch: 0082 loss_train: 0.9571 acc_train: 0.7714 loss_val: 1.0937 acc_val: 0.8400 time: 0.1023s\n",
      "Epoch: 0083 loss_train: 1.0634 acc_train: 0.7500 loss_val: 1.0869 acc_val: 0.8400 time: 0.1050s\n",
      "Epoch: 0084 loss_train: 0.9854 acc_train: 0.8143 loss_val: 1.0801 acc_val: 0.8400 time: 0.1030s\n",
      "Epoch: 0085 loss_train: 1.0638 acc_train: 0.7714 loss_val: 1.0731 acc_val: 0.8400 time: 0.1010s\n",
      "Epoch: 0086 loss_train: 1.1149 acc_train: 0.7571 loss_val: 1.0662 acc_val: 0.8400 time: 0.1061s\n",
      "Epoch: 0087 loss_train: 1.0581 acc_train: 0.7500 loss_val: 1.0596 acc_val: 0.8400 time: 0.1025s\n",
      "Epoch: 0088 loss_train: 0.9987 acc_train: 0.7286 loss_val: 1.0532 acc_val: 0.8367 time: 0.1055s\n",
      "Epoch: 0089 loss_train: 1.0166 acc_train: 0.7429 loss_val: 1.0472 acc_val: 0.8400 time: 0.1040s\n",
      "Epoch: 0090 loss_train: 0.9474 acc_train: 0.8000 loss_val: 1.0410 acc_val: 0.8400 time: 0.1041s\n",
      "Epoch: 0091 loss_train: 1.0567 acc_train: 0.7714 loss_val: 1.0348 acc_val: 0.8400 time: 0.1055s\n",
      "Epoch: 0092 loss_train: 1.0484 acc_train: 0.7357 loss_val: 1.0289 acc_val: 0.8400 time: 0.1020s\n",
      "Epoch: 0093 loss_train: 0.9219 acc_train: 0.8000 loss_val: 1.0231 acc_val: 0.8433 time: 0.1049s\n",
      "Epoch: 0094 loss_train: 0.9459 acc_train: 0.7286 loss_val: 1.0172 acc_val: 0.8433 time: 0.1069s\n",
      "Epoch: 0095 loss_train: 0.9273 acc_train: 0.7643 loss_val: 1.0114 acc_val: 0.8433 time: 0.1021s\n",
      "Epoch: 0096 loss_train: 0.9915 acc_train: 0.7357 loss_val: 1.0057 acc_val: 0.8433 time: 0.1070s\n",
      "Epoch: 0097 loss_train: 0.9318 acc_train: 0.7643 loss_val: 1.0003 acc_val: 0.8433 time: 0.1010s\n",
      "Epoch: 0098 loss_train: 0.9286 acc_train: 0.8000 loss_val: 0.9950 acc_val: 0.8433 time: 0.1088s\n",
      "Epoch: 0099 loss_train: 0.9140 acc_train: 0.7643 loss_val: 0.9898 acc_val: 0.8433 time: 0.1059s\n",
      "Epoch: 0100 loss_train: 0.9694 acc_train: 0.7571 loss_val: 0.9851 acc_val: 0.8433 time: 0.1069s\n",
      "Epoch: 0101 loss_train: 1.0551 acc_train: 0.7143 loss_val: 0.9806 acc_val: 0.8400 time: 0.1044s\n",
      "Epoch: 0102 loss_train: 0.9186 acc_train: 0.8214 loss_val: 0.9765 acc_val: 0.8433 time: 0.1094s\n",
      "Epoch: 0103 loss_train: 0.8279 acc_train: 0.8000 loss_val: 0.9723 acc_val: 0.8467 time: 0.1030s\n",
      "Epoch: 0104 loss_train: 0.9027 acc_train: 0.8000 loss_val: 0.9684 acc_val: 0.8433 time: 0.1060s\n",
      "Epoch: 0105 loss_train: 0.9650 acc_train: 0.7286 loss_val: 0.9647 acc_val: 0.8467 time: 0.1020s\n",
      "Epoch: 0106 loss_train: 0.9523 acc_train: 0.7429 loss_val: 0.9612 acc_val: 0.8467 time: 0.1009s\n",
      "Epoch: 0107 loss_train: 0.8687 acc_train: 0.8214 loss_val: 0.9576 acc_val: 0.8400 time: 0.1056s\n",
      "Epoch: 0108 loss_train: 1.0035 acc_train: 0.7500 loss_val: 0.9542 acc_val: 0.8400 time: 0.1064s\n",
      "Epoch: 0109 loss_train: 0.8950 acc_train: 0.8000 loss_val: 0.9509 acc_val: 0.8367 time: 0.1030s\n",
      "Epoch: 0110 loss_train: 0.8288 acc_train: 0.7857 loss_val: 0.9474 acc_val: 0.8333 time: 0.0951s\n",
      "Epoch: 0111 loss_train: 0.8993 acc_train: 0.8143 loss_val: 0.9439 acc_val: 0.8333 time: 0.0973s\n",
      "Epoch: 0112 loss_train: 0.8710 acc_train: 0.7571 loss_val: 0.9404 acc_val: 0.8333 time: 0.1099s\n",
      "Epoch: 0113 loss_train: 0.9141 acc_train: 0.8071 loss_val: 0.9370 acc_val: 0.8300 time: 0.1051s\n",
      "Epoch: 0114 loss_train: 0.9235 acc_train: 0.7714 loss_val: 0.9336 acc_val: 0.8300 time: 0.1059s\n",
      "Epoch: 0115 loss_train: 0.8995 acc_train: 0.7571 loss_val: 0.9302 acc_val: 0.8300 time: 0.1049s\n",
      "Epoch: 0116 loss_train: 0.8733 acc_train: 0.7714 loss_val: 0.9269 acc_val: 0.8300 time: 0.1040s\n",
      "Epoch: 0117 loss_train: 0.8540 acc_train: 0.8214 loss_val: 0.9238 acc_val: 0.8333 time: 0.1095s\n",
      "Epoch: 0118 loss_train: 0.9211 acc_train: 0.8000 loss_val: 0.9206 acc_val: 0.8300 time: 0.1040s\n",
      "Epoch: 0119 loss_train: 0.8249 acc_train: 0.7714 loss_val: 0.9171 acc_val: 0.8300 time: 0.1021s\n",
      "Epoch: 0120 loss_train: 0.8585 acc_train: 0.8071 loss_val: 0.9137 acc_val: 0.8300 time: 0.1049s\n",
      "Epoch: 0121 loss_train: 0.8075 acc_train: 0.8071 loss_val: 0.9105 acc_val: 0.8300 time: 0.1020s\n",
      "Epoch: 0122 loss_train: 0.8591 acc_train: 0.7857 loss_val: 0.9071 acc_val: 0.8267 time: 0.1093s\n",
      "Epoch: 0123 loss_train: 0.9152 acc_train: 0.7429 loss_val: 0.9037 acc_val: 0.8267 time: 0.1078s\n",
      "Epoch: 0124 loss_train: 0.8111 acc_train: 0.8000 loss_val: 0.9003 acc_val: 0.8267 time: 0.1029s\n",
      "Epoch: 0125 loss_train: 0.7962 acc_train: 0.8143 loss_val: 0.8967 acc_val: 0.8267 time: 0.1054s\n",
      "Epoch: 0126 loss_train: 1.0218 acc_train: 0.7357 loss_val: 0.8938 acc_val: 0.8267 time: 0.1028s\n",
      "Epoch: 0127 loss_train: 0.9191 acc_train: 0.7571 loss_val: 0.8911 acc_val: 0.8267 time: 0.1107s\n",
      "Epoch: 0128 loss_train: 0.7758 acc_train: 0.8429 loss_val: 0.8884 acc_val: 0.8267 time: 0.1045s\n",
      "Epoch: 0129 loss_train: 0.9684 acc_train: 0.7000 loss_val: 0.8856 acc_val: 0.8267 time: 0.0981s\n",
      "Epoch: 0130 loss_train: 0.8091 acc_train: 0.8286 loss_val: 0.8827 acc_val: 0.8300 time: 0.1010s\n",
      "Epoch: 0131 loss_train: 0.7926 acc_train: 0.8571 loss_val: 0.8797 acc_val: 0.8300 time: 0.1088s\n",
      "Epoch: 0132 loss_train: 0.7829 acc_train: 0.8357 loss_val: 0.8770 acc_val: 0.8300 time: 0.1065s\n",
      "Epoch: 0133 loss_train: 0.8674 acc_train: 0.7857 loss_val: 0.8744 acc_val: 0.8300 time: 0.1038s\n",
      "Epoch: 0134 loss_train: 0.8842 acc_train: 0.7571 loss_val: 0.8717 acc_val: 0.8267 time: 0.0981s\n",
      "Epoch: 0135 loss_train: 0.8448 acc_train: 0.7571 loss_val: 0.8690 acc_val: 0.8233 time: 0.1060s\n",
      "Epoch: 0136 loss_train: 0.8166 acc_train: 0.8000 loss_val: 0.8663 acc_val: 0.8233 time: 0.1040s\n",
      "Epoch: 0137 loss_train: 0.8223 acc_train: 0.8071 loss_val: 0.8635 acc_val: 0.8233 time: 0.1064s\n",
      "Epoch: 0138 loss_train: 0.7794 acc_train: 0.8571 loss_val: 0.8610 acc_val: 0.8233 time: 0.1050s\n",
      "Epoch: 0139 loss_train: 0.8960 acc_train: 0.7286 loss_val: 0.8585 acc_val: 0.8267 time: 0.1011s\n",
      "Epoch: 0140 loss_train: 0.8388 acc_train: 0.8071 loss_val: 0.8560 acc_val: 0.8267 time: 0.1001s\n",
      "Epoch: 0141 loss_train: 0.8680 acc_train: 0.7571 loss_val: 0.8535 acc_val: 0.8267 time: 0.1100s\n",
      "Epoch: 0142 loss_train: 0.9153 acc_train: 0.7643 loss_val: 0.8511 acc_val: 0.8267 time: 0.1027s\n",
      "Epoch: 0143 loss_train: 0.7994 acc_train: 0.8143 loss_val: 0.8486 acc_val: 0.8267 time: 0.1036s\n",
      "Epoch: 0144 loss_train: 0.8114 acc_train: 0.7500 loss_val: 0.8461 acc_val: 0.8300 time: 0.1010s\n",
      "Epoch: 0145 loss_train: 0.7955 acc_train: 0.8143 loss_val: 0.8438 acc_val: 0.8300 time: 0.1062s\n",
      "Epoch: 0146 loss_train: 0.9080 acc_train: 0.7643 loss_val: 0.8414 acc_val: 0.8367 time: 0.1092s\n",
      "Epoch: 0147 loss_train: 0.8417 acc_train: 0.7714 loss_val: 0.8390 acc_val: 0.8367 time: 0.0997s\n",
      "Epoch: 0148 loss_train: 0.8347 acc_train: 0.8000 loss_val: 0.8369 acc_val: 0.8367 time: 0.1050s\n",
      "Epoch: 0149 loss_train: 0.8532 acc_train: 0.7929 loss_val: 0.8349 acc_val: 0.8367 time: 0.1030s\n",
      "Epoch: 0150 loss_train: 0.8017 acc_train: 0.8214 loss_val: 0.8328 acc_val: 0.8367 time: 0.1014s\n",
      "Epoch: 0151 loss_train: 0.7017 acc_train: 0.8143 loss_val: 0.8309 acc_val: 0.8333 time: 0.1044s\n",
      "Epoch: 0152 loss_train: 0.7967 acc_train: 0.7857 loss_val: 0.8291 acc_val: 0.8333 time: 0.0986s\n",
      "Epoch: 0153 loss_train: 0.7516 acc_train: 0.8214 loss_val: 0.8271 acc_val: 0.8333 time: 0.1045s\n",
      "Epoch: 0154 loss_train: 0.8526 acc_train: 0.7714 loss_val: 0.8250 acc_val: 0.8333 time: 0.1026s\n",
      "Epoch: 0155 loss_train: 0.8873 acc_train: 0.7429 loss_val: 0.8230 acc_val: 0.8333 time: 0.1060s\n",
      "Epoch: 0156 loss_train: 0.8119 acc_train: 0.7643 loss_val: 0.8210 acc_val: 0.8333 time: 0.1039s\n",
      "Epoch: 0157 loss_train: 0.8037 acc_train: 0.8000 loss_val: 0.8190 acc_val: 0.8333 time: 0.1020s\n",
      "Epoch: 0158 loss_train: 0.7131 acc_train: 0.8143 loss_val: 0.8171 acc_val: 0.8333 time: 0.1090s\n",
      "Epoch: 0159 loss_train: 0.7819 acc_train: 0.7857 loss_val: 0.8152 acc_val: 0.8333 time: 0.1104s\n",
      "Epoch: 0160 loss_train: 0.7917 acc_train: 0.7500 loss_val: 0.8133 acc_val: 0.8333 time: 0.1022s\n",
      "Epoch: 0161 loss_train: 0.8251 acc_train: 0.8000 loss_val: 0.8116 acc_val: 0.8333 time: 0.1016s\n",
      "Epoch: 0162 loss_train: 0.8765 acc_train: 0.7643 loss_val: 0.8100 acc_val: 0.8333 time: 0.0976s\n",
      "Epoch: 0163 loss_train: 0.7697 acc_train: 0.8143 loss_val: 0.8085 acc_val: 0.8333 time: 0.0989s\n",
      "Epoch: 0164 loss_train: 0.8333 acc_train: 0.7857 loss_val: 0.8071 acc_val: 0.8333 time: 0.1079s\n",
      "Epoch: 0165 loss_train: 0.7359 acc_train: 0.7714 loss_val: 0.8056 acc_val: 0.8367 time: 0.0981s\n",
      "Epoch: 0166 loss_train: 0.7613 acc_train: 0.8143 loss_val: 0.8042 acc_val: 0.8367 time: 0.1033s\n",
      "Epoch: 0167 loss_train: 0.8393 acc_train: 0.8071 loss_val: 0.8028 acc_val: 0.8300 time: 0.0985s\n",
      "Epoch: 0168 loss_train: 0.7574 acc_train: 0.8071 loss_val: 0.8015 acc_val: 0.8300 time: 0.1060s\n",
      "Epoch: 0169 loss_train: 0.7914 acc_train: 0.7929 loss_val: 0.8000 acc_val: 0.8300 time: 0.1072s\n",
      "Epoch: 0170 loss_train: 0.8264 acc_train: 0.7714 loss_val: 0.7988 acc_val: 0.8300 time: 0.1020s\n",
      "Epoch: 0171 loss_train: 0.7705 acc_train: 0.7643 loss_val: 0.7978 acc_val: 0.8267 time: 0.1032s\n",
      "Epoch: 0172 loss_train: 0.9153 acc_train: 0.7714 loss_val: 0.7969 acc_val: 0.8267 time: 0.1001s\n",
      "Epoch: 0173 loss_train: 0.8228 acc_train: 0.8000 loss_val: 0.7960 acc_val: 0.8267 time: 0.1089s\n",
      "Epoch: 0174 loss_train: 0.8757 acc_train: 0.7714 loss_val: 0.7951 acc_val: 0.8233 time: 0.1039s\n",
      "Epoch: 0175 loss_train: 0.7946 acc_train: 0.7857 loss_val: 0.7943 acc_val: 0.8233 time: 0.1025s\n",
      "Epoch: 0176 loss_train: 0.7813 acc_train: 0.7857 loss_val: 0.7935 acc_val: 0.8233 time: 0.1035s\n",
      "Epoch: 0177 loss_train: 0.7674 acc_train: 0.8143 loss_val: 0.7927 acc_val: 0.8233 time: 0.1060s\n",
      "Epoch: 0178 loss_train: 0.7459 acc_train: 0.8071 loss_val: 0.7918 acc_val: 0.8233 time: 0.1059s\n",
      "Epoch: 0179 loss_train: 0.8247 acc_train: 0.7786 loss_val: 0.7906 acc_val: 0.8233 time: 0.1040s\n",
      "Epoch: 0180 loss_train: 0.8874 acc_train: 0.7286 loss_val: 0.7896 acc_val: 0.8233 time: 0.1014s\n",
      "Epoch: 0181 loss_train: 0.8330 acc_train: 0.7500 loss_val: 0.7885 acc_val: 0.8233 time: 0.1021s\n",
      "Epoch: 0182 loss_train: 0.7946 acc_train: 0.7857 loss_val: 0.7874 acc_val: 0.8233 time: 0.1095s\n",
      "Epoch: 0183 loss_train: 0.7173 acc_train: 0.7929 loss_val: 0.7863 acc_val: 0.8233 time: 0.1029s\n",
      "Epoch: 0184 loss_train: 0.7657 acc_train: 0.7714 loss_val: 0.7852 acc_val: 0.8300 time: 0.1035s\n",
      "Epoch: 0185 loss_train: 0.6405 acc_train: 0.8357 loss_val: 0.7840 acc_val: 0.8333 time: 0.1054s\n",
      "Epoch: 0186 loss_train: 0.6819 acc_train: 0.8286 loss_val: 0.7825 acc_val: 0.8333 time: 0.1000s\n",
      "Epoch: 0187 loss_train: 0.7474 acc_train: 0.8286 loss_val: 0.7808 acc_val: 0.8333 time: 0.1050s\n",
      "Epoch: 0188 loss_train: 0.7485 acc_train: 0.8143 loss_val: 0.7792 acc_val: 0.8333 time: 0.1080s\n",
      "Epoch: 0189 loss_train: 0.7287 acc_train: 0.8214 loss_val: 0.7777 acc_val: 0.8333 time: 0.1010s\n",
      "Epoch: 0190 loss_train: 0.8120 acc_train: 0.7714 loss_val: 0.7764 acc_val: 0.8333 time: 0.1030s\n",
      "Epoch: 0191 loss_train: 0.8387 acc_train: 0.7500 loss_val: 0.7753 acc_val: 0.8333 time: 0.1001s\n",
      "Epoch: 0192 loss_train: 0.8783 acc_train: 0.7714 loss_val: 0.7744 acc_val: 0.8333 time: 0.1094s\n",
      "Epoch: 0193 loss_train: 0.7951 acc_train: 0.7429 loss_val: 0.7736 acc_val: 0.8333 time: 0.1069s\n",
      "Epoch: 0194 loss_train: 0.8314 acc_train: 0.7643 loss_val: 0.7727 acc_val: 0.8333 time: 0.0991s\n",
      "Epoch: 0195 loss_train: 0.7922 acc_train: 0.7929 loss_val: 0.7720 acc_val: 0.8333 time: 0.1030s\n",
      "Epoch: 0196 loss_train: 0.6718 acc_train: 0.8357 loss_val: 0.7713 acc_val: 0.8333 time: 0.1014s\n",
      "Epoch: 0197 loss_train: 0.8063 acc_train: 0.7500 loss_val: 0.7705 acc_val: 0.8333 time: 0.1090s\n",
      "Epoch: 0198 loss_train: 0.7731 acc_train: 0.7643 loss_val: 0.7697 acc_val: 0.8333 time: 0.1021s\n",
      "Epoch: 0199 loss_train: 0.7972 acc_train: 0.8143 loss_val: 0.7686 acc_val: 0.8333 time: 0.0981s\n",
      "Epoch: 0200 loss_train: 0.8144 acc_train: 0.7643 loss_val: 0.7676 acc_val: 0.8333 time: 0.1035s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 23.8148s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = param[\"epochs\"] + 1\n",
    "best_epoch = 0\n",
    "output_dir = '../output'  # 上一级目录的output文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果目录不存在，则创建目录\n",
    "\n",
    "for epoch in range(param[\"epochs\"]):\n",
    "    loss_values.append(train(epoch))\n",
    "    # 保存模型\n",
    "    torch.save(model_transductive.state_dict(), os.path.join(output_dir, '{}.pkl'.format(epoch)))\n",
    "    # 如果验证集的损失值小于最好的损失值，就更新最好的损失值\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "    # 如果连续bad_counter次验证集的损失值都没有更新，就停止训练\n",
    "    if bad_counter == param[\"patience\"]:\n",
    "        break\n",
    "\n",
    "    file = glob.glob(os.path.join(output_dir, '*.pkl'))\n",
    "    for f in file:\n",
    "        epoch_nb = int(os.path.basename(f).split('.')[0])\n",
    "        if epoch_nb < best_epoch:  # 每个epoch都只会保留最好的模型，把在最好模型之前的模型都删除\n",
    "            os.remove(f)\n",
    "\n",
    "files = glob.glob(os.path.join(output_dir, '*.pkl'))\n",
    "for f in files:\n",
    "    epoch_nb = int(os.path.basename(f).split('.')[0])\n",
    "    if epoch_nb > best_epoch:  # 训练结束以后删除epoch在最好模型之后的模型，因为这些模型都是无用的\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# 加载最好的模型\n",
    "model_transductive.load_state_dict(torch.load(os.path.join(output_dir, '{}.pkl'.format(best_epoch))))\n",
    "# 画出损失值的变化\n",
    "plt.plot(loss_values, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results for dataset cora: loss= 0.8177 accuracy= 0.8380\n"
     ]
    }
   ],
   "source": [
    "test()  # 测试模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
